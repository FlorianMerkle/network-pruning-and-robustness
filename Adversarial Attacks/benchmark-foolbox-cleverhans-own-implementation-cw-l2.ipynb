{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from absl import app, flags\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox as fb\n",
    "from cleverhans.tf2.attacks import projected_gradient_descent, fast_gradient_method, carlini_wagner_l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.models.TensorFlowModel(model, bounds=(0,1))\n",
    "attack = fb.attacks.L2CarliniWagnerAttack(binary_search_steps = 10,\n",
    "        steps = 10000,\n",
    "        stepsize = 0.03,\n",
    "        confidence = 0,\n",
    "        initial_const = .1,\n",
    "        abort_early = False)\n",
    "adv_foolbox, clipped_adversarials, success = attack(\n",
    "    fmodel,\n",
    "    x_to_attack,\n",
    "    y_to_attack,\n",
    "    epsilons=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inputs = tf.reshape(x_to_attack, [10,28,28,1]);\n",
    "\n",
    "adv_cleverhans = carlini_wagner_l2(model,clean_inputs,y=None,\n",
    "               batch_size=10,\n",
    "               clip_min=0.,\n",
    "               clip_max=1.,\n",
    "               binary_search_steps=10,\n",
    "               max_iterations=10000,\n",
    "               abort_early=False,\n",
    "               confidence=0.,\n",
    "               initial_const=.1,\n",
    "               learning_rate=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv_eigene = cw_2_eigene_implementierung(model,x_to_attack, steps=2000, const=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für Foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l2_fb = [tf.norm(adv_foolbox[i]-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für Cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_ch = [tf.norm(adv_cleverhans[i].flatten()-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für eigene Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_own = [tf.norm(adv_eigene[i]-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [x -.3 for x in range(10)]\n",
    "y = [x +.3 for x in range(10)]\n",
    "ax = plt.subplot(111)\n",
    "w = 0.3\n",
    "ax.bar(range(10),l2_ch,width=w, color='b', align='center',label='Cleverhans')\n",
    "ax.bar(y,l2_own, width=w, color='g', align='center',label='own implementation')\n",
    "ax.bar(z,l2_fb, width=w, color='r', align='center', label='foolbox')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(labels)\n",
    "ax.set(title='CW2 Implementation Benchmark',ylabel='L2 Distance', xlabel='Sample No')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fmodel(x_to_attack[:10])\n",
    "adv_fb_res = fmodel(adv_foolbox)\n",
    "adv_ch_res = fmodel(adv_cleverhans)\n",
    "adv_eigen_res = fmodel(np.array(adv_eigene))\n",
    "\n",
    "print(y_to_attack.numpy().tolist())\n",
    "print([np.argmax(x) for x in res])\n",
    "print([np.argmax(x) for x in adv_fb_res])\n",
    "print([np.argmax(x) for x in adv_ch_res])\n",
    "print([np.argmax(x) for x in adv_eigen_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-5 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'conv_1': (5, 5, 1, 6),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'conv_2': (5, 5, 6, 16),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'conv_3': (1, 1, 16, 120),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'dense_1': (5*5*16, 120),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'dense_2': (120, 84),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'dense_3': (84, 10),\n",
    "}\n",
    "bias_shapes = {\n",
    "    #output depth\n",
    "    'conv_1': (6),\n",
    "    'conv_2': (16),\n",
    "    'dense_1': (120),\n",
    "    'dense_2': (84),\n",
    "    'dense_3': (10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)\n",
    "        x = tf.nn.bias_add(x, self.b,)\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "\n",
    "        \n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "\n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        if self.a == None:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, None)\n",
    "        self.pre_softmax = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        self.pre_softmax = x\n",
    "        return tf.nn.softmax(x)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[:10].reshape(10,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[:10]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_base_model(999,'')\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_2_eigene_implementierung(model, xs, const=1, steps=1000, learning_rate=.03, ):\n",
    "    xa=[]\n",
    "    \n",
    "    for x in xs:\n",
    "        w = tf.Variable(initial_value=tf.zeros(x.shape))\n",
    "        \n",
    "        show_adv = []\n",
    "        optimizer = tf.keras.optimizers.Adam(.03)\n",
    "        target = find_second_most_probable_class(get_logits(model,x))\n",
    "        adv_image = .5 * (tf.tanh(w) + 1)\n",
    "        best_adv_image = adv_image\n",
    "        best_l2_dist = tf.norm(adv_image - x).numpy()\n",
    "        for i in tqdm(range(steps)):\n",
    "            with tf.GradientTape() as tape:\n",
    "                adv_image = .5 * (tf.tanh(w) + 1)\n",
    "                preds = model(adv_image)\n",
    "                loss = loss_function(model= model, x=x, adv_image=adv_image, const=const, target=target)\n",
    "                l2_distance = tf.norm(adv_image - x).numpy()\n",
    "                if l2_distance < best_l2_dist and np.argmax(preds) == target:\n",
    "                    best_adv_image = adv_image\n",
    "                    best_l2_dist = l2_distance\n",
    "                grads = tape.gradient(loss, w)\n",
    "                optimizer.apply_gradients([(grads, w)])\n",
    "        xa.append(best_adv_image)\n",
    "    return xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, x, adv_image, const, target):\n",
    "    l1 = tf.square(tf.norm(adv_image - x))\n",
    "    logit_of_best_other = get_logit_of_best_except_target(model, adv_image, target)\n",
    "    logit_of_target = get_logit_of_target(model, adv_image,target)\n",
    "    l2 = const * tf.math.maximum(\n",
    "        logit_of_best_other\n",
    "        - logit_of_target\n",
    "        , 0 )\n",
    "    return l1 + l2\n",
    "\n",
    "\n",
    "def find_second_most_probable_class(logits):\n",
    "    logits = logits.numpy().flatten()\n",
    "    return np.argpartition(logits,len(logits)-2)[len(logits)-2]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_logit_of_best_except_target(model, adv_x, target):\n",
    "    all_logits = get_logits(model, adv_x)\n",
    "    #print('all_logits', all_logits)\n",
    "    most_probable_class = tf.math.argmax(all_logits)\n",
    "    second_most_probable_class = find_second_most_probable_class(all_logits)\n",
    "    if target == most_probable_class: \n",
    "        return all_logits[second_most_probable_class ]\n",
    "    if target != most_probable_class: \n",
    "        return all_logits[most_probable_class ]\n",
    "\n",
    "def get_logit_of_target(model, adv_x, target):\n",
    "    all_logits = get_logits(model, adv_x)\n",
    "    return all_logits[target]\n",
    "\n",
    "def get_logits(model, x):\n",
    "    model(x)\n",
    "    return model.pre_softmax[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
