{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/brendel_bethge.py:781: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec=[])\n",
      "/home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/brendel_bethge.py:1358: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec=spec)\n",
      "/home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/brendel_bethge.py:1554: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec=spec)\n",
      "/home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/brendel_bethge.py:1706: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec=spec)\n",
      "/home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/brendel_bethge.py:1900: NumbaDeprecationWarning: The 'numba.jitclass' decorator has moved to 'numba.experimental.jitclass' to better reflect the experimental nature of the functionality. Please update your imports to accommodate this change and see http://numba.pydata.org/numba-doc/latest/reference/deprecation.html#change-of-jitclass-location for the time frame.\n",
      "  @jitclass(spec=spec)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import app, flags\n",
    "from easydict import EasyDict\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dense, Flatten, Conv2D, MaxPool2D\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from cleverhans.future.tf2.attacks import projected_gradient_descent, fast_gradient_method\n",
    "\n",
    "import foolbox as fb\n",
    "#import foolbox.ext.native as fbn\n",
    "import eagerpy as ep\n",
    "#from foolbox import TensorFlowModel, accuracy, samples\n",
    "import foolbox.attacks as fa\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "#print(x_train[:100])\n",
    "x_train = (x_train.reshape(60000, 784).astype('float32') / 255)\n",
    "x_test = (x_test.reshape(10000, 784).astype('float32') / 255)\n",
    "#x = tf.convert_to_tensor(tf.expand_dims(x_train[:100].reshape(28,28,1), axis=0), tf.float32)\n",
    "x = tf.convert_to_tensor(x_train[:35].reshape(35,28,28,1))\n",
    "y = tf.convert_to_tensor([y_train[:35]])[0];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model = tf.keras.models.load_model('../saved-models/attack-test-model')\n",
    "fmodel = fb.models.TensorFlowModel(model, bounds=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "lrs = [1e1,1e3, 1e5, 1e7, 1e9, 1e11, 1e13,1e15]\n",
    "for lr in lrs:\n",
    "\n",
    "    fmodel = fb.models.TensorFlowModel(model, bounds=(0,1))\n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(steps = 1000, lr=lr, binary_search_steps=100)\n",
    "    advs, clipped_advs, success = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    if all(list(success.numpy()[0])):\n",
    "        sum_of_dist = 0\n",
    "        l0_dist = np.sum([np.count_nonzero(x[i]-advs[0][i]) for i in range(35)])\n",
    "        res.append(l0_dist)\n",
    "    else:\n",
    "        print('not sucessful', lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 46, 47, 31, 37, 49, 36, 36]\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30.6, 18.857142857142858, 13.257142857142858, 15.8, 14.314285714285715, 15.342857142857143, 13.771428571428572, 14.657142857142857]\n"
     ]
    }
   ],
   "source": [
    "print([x/35 for x in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f507be73dc0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(advs[0][2], (28,28)))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f51a83aa400>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(x[0], (28,28)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(28, 28, 1), dtype=float32, numpy=\n",
       "array([[[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18393044],\n",
       "        [0.22288385],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.2324681 ],\n",
       "        [0.        ],\n",
       "        [0.19526619],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21340184],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.18813065],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19999252],\n",
       "        [0.1832843 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.21235274],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.23615874],\n",
       "        [0.        ],\n",
       "        [0.20520392],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18110551],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.21282198],\n",
       "        [0.        ],\n",
       "        [0.19493413],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18200673],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20331159],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19495264],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.21681997],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22139229],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18946762],\n",
       "        [0.        ],\n",
       "        [0.22817847],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.24086782],\n",
       "        [0.19561347],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18731746],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.01176471],\n",
       "        [0.2846758 ],\n",
       "        [0.3024859 ],\n",
       "        [0.07058824],\n",
       "        [0.49411765],\n",
       "        [0.53333336],\n",
       "        [0.6862745 ],\n",
       "        [0.33067912],\n",
       "        [0.6509804 ],\n",
       "        [0.78045326],\n",
       "        [0.96862745],\n",
       "        [0.49803922],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20765994]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.1916362 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.11764706],\n",
       "        [0.14117648],\n",
       "        [0.36862746],\n",
       "        [0.6039216 ],\n",
       "        [0.6666667 ],\n",
       "        [0.7539545 ],\n",
       "        [0.75582963],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.88235295],\n",
       "        [0.6745098 ],\n",
       "        [0.793312  ],\n",
       "        [0.9490196 ],\n",
       "        [0.5637549 ],\n",
       "        [0.2509804 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19475979],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.2218102 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19215687],\n",
       "        [0.93333334],\n",
       "        [0.78837526],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.9843137 ],\n",
       "        [0.3647059 ],\n",
       "        [0.32156864],\n",
       "        [0.32156864],\n",
       "        [0.4031915 ],\n",
       "        [0.15294118],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.21103789],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.24758102],\n",
       "        [0.        ],\n",
       "        [0.25302196],\n",
       "        [0.85882354],\n",
       "        [0.99215686],\n",
       "        [0.77474236],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.7684383 ],\n",
       "        [0.7764706 ],\n",
       "        [0.7137255 ],\n",
       "        [0.96862745],\n",
       "        [0.94509804],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20600061],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19217496]],\n",
       "\n",
       "       [[0.23092918],\n",
       "        [0.23017885],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.3137255 ],\n",
       "        [0.42975557],\n",
       "        [0.41960785],\n",
       "        [0.99215686],\n",
       "        [0.7869694 ],\n",
       "        [0.8039216 ],\n",
       "        [0.04313726],\n",
       "        [0.        ],\n",
       "        [0.16862746],\n",
       "        [0.6039216 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.24981675],\n",
       "        [0.22346295],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.05490196],\n",
       "        [0.00392157],\n",
       "        [0.6039216 ],\n",
       "        [0.99215686],\n",
       "        [0.3529412 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.260737  ],\n",
       "        [0.15908107],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.54509807],\n",
       "        [0.99215686],\n",
       "        [0.74509805],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19250023],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.1957271 ],\n",
       "        [0.        ],\n",
       "        [0.22645944],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.21617585],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20872225],\n",
       "        [0.        ],\n",
       "        [0.04313726],\n",
       "        [0.52758807],\n",
       "        [0.99215686],\n",
       "        [0.46480364],\n",
       "        [0.19262466],\n",
       "        [0.        ],\n",
       "        [0.18228647],\n",
       "        [0.20235035],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19378397],\n",
       "        [0.        ],\n",
       "        [0.20486084],\n",
       "        [0.13725491],\n",
       "        [0.94509804],\n",
       "        [0.88235295],\n",
       "        [0.627451  ],\n",
       "        [0.42352942],\n",
       "        [0.18613546],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18783814]],\n",
       "\n",
       "       [[0.2008377 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.229756  ],\n",
       "        [0.        ],\n",
       "        [0.21771163],\n",
       "        [0.        ],\n",
       "        [0.31764707],\n",
       "        [0.9411765 ],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.46666667],\n",
       "        [0.29185805],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22149526],\n",
       "        [0.22274911],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.22212423],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18312353],\n",
       "        [0.        ],\n",
       "        [0.25320688],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.1764706 ],\n",
       "        [0.7294118 ],\n",
       "        [0.7529996 ],\n",
       "        [0.7812039 ],\n",
       "        [0.5882353 ],\n",
       "        [0.10588235],\n",
       "        [0.        ],\n",
       "        [0.23941067],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22679248],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.23826602],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20112872],\n",
       "        [0.        ],\n",
       "        [0.20107359],\n",
       "        [0.        ],\n",
       "        [0.2404598 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.24454057],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.0627451 ],\n",
       "        [0.3647059 ],\n",
       "        [0.7990542 ],\n",
       "        [0.99215686],\n",
       "        [0.73333335],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.23100424],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.21564911],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20347394],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.9764706 ],\n",
       "        [0.7858132 ],\n",
       "        [0.75744164],\n",
       "        [0.2509804 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.19642358],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22249831],\n",
       "        [0.        ],\n",
       "        [0.22213195],\n",
       "        [0.20263806],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19233672],\n",
       "        [0.2027542 ],\n",
       "        [0.36146232],\n",
       "        [0.50980395],\n",
       "        [0.7176471 ],\n",
       "        [0.80799186],\n",
       "        [0.99215686],\n",
       "        [0.8117647 ],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18490635],\n",
       "        [0.21591496],\n",
       "        [0.        ],\n",
       "        [0.15294118],\n",
       "        [0.5803922 ],\n",
       "        [0.8980392 ],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.98039216],\n",
       "        [0.7137255 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.18099026]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.21518476],\n",
       "        [0.        ],\n",
       "        [0.22127569],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.1941565 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.09411765],\n",
       "        [0.44705883],\n",
       "        [0.64259887],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.7882353 ],\n",
       "        [0.50533086],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21900113],\n",
       "        [0.        ],\n",
       "        [0.22215235],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20331964]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.22672851],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22129497],\n",
       "        [0.        ],\n",
       "        [0.19695604],\n",
       "        [0.27186555],\n",
       "        [0.25882354],\n",
       "        [0.8352941 ],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.7764706 ],\n",
       "        [0.31764707],\n",
       "        [0.00784314],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20626207],\n",
       "        [0.20876189],\n",
       "        [0.        ],\n",
       "        [0.1942797 ],\n",
       "        [0.        ],\n",
       "        [0.23186782],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.21833795],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.07058824],\n",
       "        [0.67058825],\n",
       "        [0.85882354],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.7647059 ],\n",
       "        [0.3137255 ],\n",
       "        [0.24393846],\n",
       "        [0.        ],\n",
       "        [0.20052369],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22629629],\n",
       "        [0.        ],\n",
       "        [0.22592957],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21568628],\n",
       "        [0.6745098 ],\n",
       "        [0.8862745 ],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.95686275],\n",
       "        [0.52156866],\n",
       "        [0.04313726],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19170539],\n",
       "        [0.        ],\n",
       "        [0.22979462],\n",
       "        [0.19886807],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21376155],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.53333336],\n",
       "        [0.99215686],\n",
       "        [0.99215686],\n",
       "        [0.7904136 ],\n",
       "        [0.83137256],\n",
       "        [0.5294118 ],\n",
       "        [0.5176471 ],\n",
       "        [0.0627451 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21854301],\n",
       "        [0.19428685],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21177739],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21398029],\n",
       "        [0.        ],\n",
       "        [0.        ]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.1850929 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20901273]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.22610852],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.19680476],\n",
       "        [0.        ],\n",
       "        [0.1845324 ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20243846],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.21523702]],\n",
       "\n",
       "       [[0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.20217316],\n",
       "        [0.18273975],\n",
       "        [0.20271122],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.23059928],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "advs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'conv_1': (5, 5, 1, 6),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'conv_2': (5, 5, 6, 16),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'conv_3': (1, 1, 16, 120),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'dense_1': (5*5*16, 120),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'dense_2': (120, 84),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'dense_3': (84, 10),\n",
    "}\n",
    "bias_shapes = {\n",
    "    #output depth\n",
    "    'conv_1': (6),\n",
    "    'conv_2': (16),\n",
    "    'dense_1': (120),\n",
    "    'dense_2': (84),\n",
    "    'dense_3': (10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "#        return tf.keras.layers.AveragePooling2D(pool_size=(self.k, self.k), strides=None, padding=self.p, data_format='channels_first')(inputs)\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "\n",
    "        \n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "\n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        if self.a == None:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, None)\n",
    "        self.pre_softmax = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        self.pre_softmax = x\n",
    "        return tf.nn.softmax(x)\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            weights = self.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        \n",
    "                        no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                        # find unpruned weights\n",
    "                        non_zero_weights = np.nonzero(flat_masks)[0]\n",
    "                        # calculate the amount of weights to be pruned this round\n",
    "                        no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                        # shuffle all non-zero weights\n",
    "                        random.shuffle(non_zero_weights)\n",
    "                        # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                        indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                        \n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self, ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = [0, 3]\n",
    "        dense_layer_to_prune = [6, 9, 12]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            #print('inside conv prune func',get_zeros_ratio(self.get_weights()))\n",
    "            weights = self.get_weights()\n",
    "            \n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        #flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                        #flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                        #print(no_of_weights_to_prune)\n",
    "                        #indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "\n",
    "\n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            \n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return\n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            all_channels = np.empty((0,5,5))\n",
    "            original_shapes = []\n",
    "            for layer_to_prune in conv_layers_to_prune:\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                all_channels = np.concatenate((all_channels, channels))\n",
    "            mask = np.ones(all_channels.shape)\n",
    "            vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(ratio * len(vals))\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                all_channels[channel_to_prune] = tf.zeros((5,5))\n",
    "                mask[channel_to_prune] = tf.zeros((5,5))\n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(conv_layers_to_prune):\n",
    "                original_shape = convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(mask[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[layer_to_prune + 2] = convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = original_shape[0]*original_shape[1]    \n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(ratio * len(vals))\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(dense_layers_to_prune):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[layer_to_prune + 2][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5956 - accuracy: 0.8845 - val_loss: 1.5096 - val_accuracy: 0.9562\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5003 - accuracy: 0.9642 - val_loss: 1.4905 - val_accuracy: 0.9721\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4892 - accuracy: 0.9742 - val_loss: 1.4914 - val_accuracy: 0.9716\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4834 - accuracy: 0.9793 - val_loss: 1.4855 - val_accuracy: 0.9768\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4802 - accuracy: 0.9822 - val_loss: 1.4841 - val_accuracy: 0.9780\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4778 - accuracy: 0.9843 - val_loss: 1.4800 - val_accuracy: 0.9820\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4764 - accuracy: 0.9856 - val_loss: 1.4802 - val_accuracy: 0.9816\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4745 - accuracy: 0.9873 - val_loss: 1.4782 - val_accuracy: 0.9832\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4731 - accuracy: 0.9887 - val_loss: 1.4808 - val_accuracy: 0.9806\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4721 - accuracy: 0.9898 - val_loss: 1.4777 - val_accuracy: 0.9840\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4716 - accuracy: 0.9901 - val_loss: 1.4761 - val_accuracy: 0.9856\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4708 - accuracy: 0.9909 - val_loss: 1.4772 - val_accuracy: 0.9845\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4701 - accuracy: 0.9916 - val_loss: 1.4762 - val_accuracy: 0.9854\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4699 - accuracy: 0.9918 - val_loss: 1.4760 - val_accuracy: 0.9856\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4697 - accuracy: 0.9918 - val_loss: 1.4753 - val_accuracy: 0.9855\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4690 - accuracy: 0.9926 - val_loss: 1.4741 - val_accuracy: 0.9874\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4686 - accuracy: 0.9929 - val_loss: 1.4745 - val_accuracy: 0.9875\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4679 - accuracy: 0.9938 - val_loss: 1.4774 - val_accuracy: 0.9844\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4680 - accuracy: 0.9936 - val_loss: 1.4764 - val_accuracy: 0.9855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomConvModel at 0x7f52de1aca30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = initialize_base_model(999,'')\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
